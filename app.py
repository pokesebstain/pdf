import os
import logging
import tempfile
from datetime import datetime
import streamlit as st
import tabula
import pandas as pd
import base64
import pdfplumber

# ç®€åŒ–æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¸¸é‡å®šä¹‰
UPLOAD_FOLDER = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'uploads')
MAX_FILE_SIZE = 32 * 1024 * 1024  # 32MB max file size

# ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

class PDFProcessor:
    def __init__(self, upload_folder):
        self.upload_folder = upload_folder
        self.temp_dir = tempfile.mkdtemp(dir=upload_folder)
        logger.info(f"Created temp directory: {self.temp_dir}")

    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if os.path.exists(self.temp_dir):
            try:
                import shutil
                shutil.rmtree(self.temp_dir)
                logger.info(f"Cleaned up temp directory: {self.temp_dir}")
            except Exception as e:
                logger.error(f"Error cleaning temp directory: {e}")

    def _clean_column_name(self, col, idx):
        """ç»Ÿä¸€åˆ—åæ¸…ç†é€»è¾‘ï¼ŒåŒ…å«å¸¸è§åˆ—åä¿®æ­£"""
        if pd.isna(col) or col == '':
            return f'åˆ—{idx+1}'
        
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ¸…ç†é¦–å°¾ç©ºæ ¼
        col_name = str(col).strip()
        
        # æ›¿æ¢æ¢è¡Œç¬¦ä¸ºç©ºæ ¼
        col_name = col_name.replace('\n', ' ').replace('\r', ' ')
        
        # æ¸…ç†è¿ç»­çš„ç©ºæ ¼
        col_name = ' '.join(col_name.split())
        
        # å¸¸è§åˆ—åä¿®æ­£å­—å…¸
        common_columns = {
            'epartmen': 'department',
            'ept': 'department',
            'dept': 'department',
            'nam': 'name',
            'nme': 'name',
            'id': 'ID',
            'num': 'number',
            'no': 'number',
            'desc': 'description',
            'qty': 'quantity',
            'amt': 'amount'
        }
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®æ­£
        col_name_lower = col_name.lower()
        for wrong, correct in common_columns.items():
            if col_name_lower == wrong or col_name_lower.startswith(wrong):
                logger.info(f"åˆ—åä¿®æ­£: {col_name} -> {correct}")
                return correct
        
        # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤åˆ—å
        return col_name or f'åˆ—{idx+1}'

    def _calculate_similarity(self, table1, table2):
        """è®¡ç®—ä¸¤ä¸ªè¡¨æ ¼çš„ç›¸ä¼¼åº¦"""
        try:
            # ç¡®ä¿ä¸¤ä¸ªè¡¨æ ¼å…·æœ‰ç›¸åŒçš„åˆ—
            common_cols = set(table1.columns) & set(table2.columns)
            if not common_cols:
                return 0.0

            # åªæ¯”è¾ƒå…±åŒçš„åˆ—
            t1 = table1[list(common_cols)].fillna('')
            t2 = table2[list(common_cols)].fillna('')

            # å°†è¡¨æ ¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ•°ç»„
            arr1 = t1.values.flatten()
            arr2 = t2.values.flatten()

            # è®¡ç®—ç›¸åŒå…ƒç´ çš„æ¯”ä¾‹
            matches = sum(1 for a, b in zip(arr1, arr2) if str(a).strip() == str(b).strip())
            total = len(arr1)

            return matches / total if total > 0 else 0.0

        except Exception as e:
            logger.error(f"è®¡ç®—è¡¨æ ¼ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {str(e)}")
            return 0.0

    def _is_duplicate_table(self, new_table, existing_tables):
        """æ£€æŸ¥è¡¨æ ¼æ˜¯å¦æ˜¯é‡å¤çš„"""
        for existing_table in existing_tables:
            # æ£€æŸ¥ç»´åº¦æ˜¯å¦ç›¸åŒ
            if new_table.shape == existing_table.shape:
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self._calculate_similarity(
                    new_table.drop(['æ¥æºæ–‡ä»¶', 'è¡¨æ ¼åºå·'], axis=1, errors='ignore'),
                    existing_table.drop(['æ¥æºæ–‡ä»¶', 'è¡¨æ ¼åºå·'], axis=1, errors='ignore')
                )
                if similarity > 0.8:  # 80%ç›¸ä¼¼åº¦è®¤ä¸ºæ˜¯é‡å¤
                    logger.debug(f"å‘ç°é‡å¤è¡¨æ ¼ï¼Œç›¸ä¼¼åº¦: {similarity:.2%}")
                    return True
        return False

    def _try_extract(self, pdf_path, header_opt):
        """å°è¯•ä½¿ç”¨æŒ‡å®šheaderå‚æ•°æå–è¡¨æ ¼"""
        try:
            logger.info(f"å¼€å§‹æå–è¡¨æ ¼: {os.path.basename(pdf_path)}, header={header_opt}")
            
            # ä½¿ç”¨latticeæ¨¡å¼æå–è¡¨æ ¼ï¼Œæ·»åŠ æ›´å¤šå‚æ•°ä»¥æé«˜è¯†åˆ«å‡†ç¡®æ€§
            params = {
                'lattice': True,
                'stream': False,
                'guess': True,  # å¯ç”¨è‡ªåŠ¨æ£€æµ‹
                'columns': None,  # è‡ªåŠ¨æ£€æµ‹åˆ—
                'pages': 'all',
                'multiple_tables': True,
                'relative_area': True,  # ä½¿ç”¨ç›¸å¯¹åŒºåŸŸ
            }
            
            valid_tables = []
            
            try:
                tables = tabula.read_pdf(
                    pdf_path,
                    pandas_options={
                        'header': header_opt
                    },
                    java_options=['-Dfile.encoding=UTF8', '-Xmx512m'],
                    encoding='utf-8',
                    silent=False,
                    **params
                )
                
                if tables:
                    logger.debug(f"æå–åˆ°{len(tables)}ä¸ªåŸå§‹è¡¨æ ¼")
                    for table in tables:
                        if table is None or table.empty:
                            continue
                            
                        if len(table.index) >= 1 and len(table.columns) >= 2:  # é™ä½æœ€å°åˆ—æ•°è¦æ±‚åˆ°2åˆ—
                            # æ¸…ç†è¡¨æ ¼æ•°æ®
                            table = (table
                                .replace({r'\r': '', r'\n': ' '}, regex=True)
                                .fillna('')
                                .dropna(how='all', axis=0)
                                .dropna(how='all', axis=1))
                            
                            if not table.empty:
                                # æ·»åŠ æ¥æºä¿¡æ¯
                                table['æ¥æºæ–‡ä»¶'] = os.path.basename(pdf_path)
                                table['è¡¨æ ¼åºå·'] = f'Table_{len(valid_tables) + 1}'
                                
                                # æ£€æŸ¥æ˜¯å¦é‡å¤è¡¨æ ¼
                                if not self._is_duplicate_table(table, valid_tables):
                                    valid_tables.append(table)
                                    logger.info(f"æ‰¾åˆ°æ–°çš„æœ‰æ•ˆè¡¨æ ¼ï¼Œå½“å‰å…±æœ‰{len(valid_tables)}ä¸ªè¡¨æ ¼")
                            
            except Exception as e:
                logger.warning(f"è¡¨æ ¼æå–å¤±è´¥: {str(e)}")
            
            # å¦‚æœlatticeæ¨¡å¼å¤±è´¥ï¼Œå°è¯•streamæ¨¡å¼ï¼ŒåŒæ ·ä½¿ç”¨ä¼˜åŒ–çš„å‚æ•°
            if not valid_tables:
                logger.info("latticeæ¨¡å¼æœªæå–åˆ°è¡¨æ ¼ï¼Œå°è¯•streamæ¨¡å¼")
                params['lattice'] = False
                params['stream'] = True
                params['guess'] = True
                params['columns'] = None  # è‡ªåŠ¨æ£€æµ‹åˆ—
                
                try:
                    tables = tabula.read_pdf(
                        pdf_path,
                        pandas_options={
                            'header': header_opt
                        },
                        java_options=['-Dfile.encoding=UTF8', '-Xmx512m'],
                        encoding='utf-8',
                        silent=False,
                        **params
                    )
                    
                    if tables:
                        for table in tables:
                            if table is None or table.empty:
                                continue
                                
                            if len(table.index) >= 1 and len(table.columns) >= 5:
                                table = (table
                                    .replace({r'\r': '', r'\n': ' '}, regex=True)
                                    .fillna('')
                                    .dropna(how='all', axis=0)
                                    .dropna(how='all', axis=1))
                                
                                if not table.empty:
                                    table['æ¥æºæ–‡ä»¶'] = os.path.basename(pdf_path)
                                    table['è¡¨æ ¼åºå·'] = f'Table_{len(valid_tables) + 1}'
                                    
                                    if not self._is_duplicate_table(table, valid_tables):
                                        valid_tables.append(table)
                                        logger.info(f"ä½¿ç”¨streamæ¨¡å¼æ‰¾åˆ°æ–°çš„æœ‰æ•ˆè¡¨æ ¼ï¼Œå½“å‰å…±æœ‰{len(valid_tables)}ä¸ªè¡¨æ ¼")
                                        
                except Exception as e:
                    logger.warning(f"streamæ¨¡å¼æå–å¤±è´¥: {str(e)}")
            
            logger.info(f"æœ€ç»ˆæå–åˆ° {len(valid_tables)} ä¸ªæœ‰æ•ˆè¡¨æ ¼")
            return valid_tables
            
        except Exception as e:
            logger.error(f"è¡¨æ ¼æå–è¿‡ç¨‹å‡ºé”™ (header={header_opt}): {e}")
            return []

    def extract_tables_from_pdf(self, pdf_path):
        """ä»PDFæ–‡ä»¶ä¸­æå–è¡¨æ ¼ï¼Œè‡ªåŠ¨å°è¯•ä¸åŒheaderå‚æ•°"""
        try:
            if not os.path.exists(pdf_path):
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
                return []
                
            file_size = os.path.getsize(pdf_path)
            if file_size == 0:
                logger.error(f"ç©ºæ–‡ä»¶: {pdf_path}")
                return []
                
            logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {os.path.basename(pdf_path)}, å¤§å°: {file_size/1024:.1f}KB")

            all_tables = self._try_extract(pdf_path, 0)
            if not all_tables:
                logger.info(f"header=0æœªæå–åˆ°æœ‰æ•ˆè¡¨æ ¼ï¼Œå°è¯•header=None: {os.path.basename(pdf_path)}")
                all_tables = self._try_extract(pdf_path, None)

            processed_tables = []
            
            if all_tables:
                for table_index, table in enumerate(all_tables):
                    try:
                        if table is None or table.empty:
                            continue
                            
                        # æ¸…ç†åˆ—å
                        new_columns = []
                        seen_columns = set()
                        for idx, col in enumerate(table.columns):
                            col_name = self._clean_column_name(col, idx)
                            
                            base_name = col_name
                            counter = 1
                            while col_name in seen_columns:
                                col_name = f"{base_name}_{counter}"
                                counter += 1
                            seen_columns.add(col_name)
                            new_columns.append(col_name)
                            
                        table.columns = new_columns
                        processed_tables.append(table)
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†è¡¨æ ¼{table_index+1}æ—¶å‡ºé”™: {str(e)}")
                        continue
                        
            if not processed_tables:
                logger.warning(f"æœªä»æ–‡ä»¶ä¸­æå–åˆ°æœ‰æ•ˆè¡¨æ ¼: {os.path.basename(pdf_path)}")
                    
            logger.info(f"æˆåŠŸä»æ–‡ä»¶æå–{len(processed_tables)}ä¸ªè¡¨æ ¼: {os.path.basename(pdf_path)}")
            return processed_tables
            
        except Exception as e:
            logger.error(f"æå–PDFè¡¨æ ¼æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return []

    def merge_tables(self, tables_data):
        """åˆå¹¶æ‰€æœ‰è¡¨æ ¼åˆ°ä¸€ä¸ªsheetä¸­ï¼Œä¿æŒæ¥æºæ–‡ä»¶å’Œè¡¨æ ¼åºå·åˆ—"""
        if not tables_data:
            return None

        try:
            # æ”¶é›†æ‰€æœ‰è¡¨æ ¼
            all_tables = []
            for tables in tables_data:
                all_tables.extend(tables)

            if not all_tables:
                logger.warning("No tables to merge")
                return None

            # åˆ›å»ºä¸€ä¸ªç©ºçš„DataFrameæ¥å­˜å‚¨æ‰€æœ‰åˆå¹¶çš„è¡¨æ ¼
            merged_data = []
            
            for table in all_tables:
                # æ¸…ç†æ•°æ®
                table = table.replace({r'\r': '', r'\n': ' '}, regex=True)
                table = table.fillna('')
                
                # ç¡®ä¿æ¥æºæ–‡ä»¶å’Œè¡¨æ ¼åºå·åˆ—åœ¨æœ€å‰é¢
                cols = list(table.columns)
                source_cols = ['æ¥æºæ–‡ä»¶', 'è¡¨æ ¼åºå·']
                other_cols = [col for col in cols if col not in source_cols]
                table = table[source_cols + other_cols]
                
                # æ·»åŠ å½“å‰è¡¨æ ¼
                merged_data.append(table)
            
            # å‚ç›´åˆå¹¶æ‰€æœ‰æ•°æ®
            final_df = pd.concat(merged_data, ignore_index=True)
            
            # åˆ›å»ºExcelæ–‡ä»¶
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = os.path.join(self.temp_dir, f'æå–çš„è¡¨æ ¼_{timestamp}.xlsx')

            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                # å†™å…¥åˆå¹¶åçš„æ•°æ®
                final_df.to_excel(writer, sheet_name='æå–çš„è¡¨æ ¼', index=False)
                
                # è·å–å·¥ä½œè¡¨å¯¹è±¡
                worksheet = writer.sheets['æå–çš„è¡¨æ ¼']
                
                # è®¾ç½®åˆ—å®½
                for idx, col in enumerate(final_df.columns):
                    max_length = max(
                        final_df[col].astype(str).apply(len).max(),
                        len(str(col))
                    )
                    worksheet.column_dimensions[chr(65 + idx)].width = min(max_length + 2, 50)

            logger.info(f"Successfully saved {len(all_tables)} tables into {output_path}")
            return output_path

        except Exception as e:
            logger.error(f"Error merging tables: {e}")
            return None

    def process_pdfs(self, files):
        """å¤„ç†å¤šä¸ªPDFæ–‡ä»¶å¹¶å°†è¡¨æ ¼åˆå¹¶åˆ°ä¸€ä¸ªExcelæ–‡ä»¶ä¸­"""
        if not files:
            return None, []

        try:
            all_tables = []
            failed_files = []

            if not isinstance(files, (list, tuple)):
                logger.error(f"æ— æ•ˆçš„è¾“å…¥ç±»å‹: {type(files)}")
                return None, [("input", f"æ— æ•ˆçš„è¾“å…¥ç±»å‹: {type(files)}")]

            for file in files:
                if not hasattr(file, 'read'):
                    logger.error(f"æ— æ•ˆçš„æ–‡ä»¶å¯¹è±¡ç±»å‹: {type(file)}")
                    failed_files.append((str(type(file)), "æ— æ•ˆçš„æ–‡ä»¶å¯¹è±¡"))
                    continue

                filename = getattr(file, 'name', getattr(file, 'filename', None))
                logger.debug(f"åŸå§‹æ–‡ä»¶å: {filename} (ç±»å‹: {type(filename)})")
                
                if filename is None:
                    logger.error("æ— æ³•è·å–æ–‡ä»¶å")
                    failed_files.append(("unknown", "æ— æ³•è·å–æ–‡ä»¶å"))
                    continue
                    
                try:
                    filename = str(filename)
                    if not isinstance(filename, str):
                        raise TypeError(f"æ–‡ä»¶åè½¬æ¢å¤±è´¥ï¼Œå¾—åˆ°ç±»å‹: {type(filename)}")
                except Exception as e:
                    logger.error(f"æ–‡ä»¶åè½¬æ¢é”™è¯¯: {str(e)}")
                    failed_files.append((str(filename), f"æ–‡ä»¶åè½¬æ¢é”™è¯¯: {str(e)}"))
                    continue
                    
                if not filename.lower().endswith('.pdf'):
                    logger.warning(f"è·³è¿‡éPDFæ–‡ä»¶: {filename}")
                    continue

                try:
                    pdf_path = os.path.join(self.temp_dir, filename)
                    logger.info(f"æ„é€ çš„è·¯å¾„: {pdf_path} (ç±»å‹: {type(pdf_path)})")
                    
                    if not isinstance(pdf_path, (str, bytes, os.PathLike)):
                        raise TypeError(f"æ— æ•ˆçš„æ–‡ä»¶è·¯å¾„ç±»å‹: {type(pdf_path)}")
                        
                    with open(pdf_path, 'wb') as f:
                        file_content = file.read()  # ä½¿ç”¨read()è€Œä¸æ˜¯getvalue()
                        if not file_content:
                            logger.warning(f"ç©ºæ–‡ä»¶å†…å®¹: {filename}")
                            failed_files.append((filename, "ç©ºæ–‡ä»¶å†…å®¹"))
                            continue
                        f.write(file_content)

                    if not os.path.exists(pdf_path):
                        logger.error(f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {filename}")
                        failed_files.append((filename, "æ–‡ä»¶ä¿å­˜å¤±è´¥"))
                        continue

                    file_size = os.path.getsize(pdf_path)
                    if file_size == 0:
                        logger.warning(f"ç©ºæ–‡ä»¶: {filename}")
                        failed_files.append((filename, "æ–‡ä»¶ä¸ºç©º"))
                        continue

                    tables = self.extract_tables_from_pdf(pdf_path)
                    if tables:
                        all_tables.append(tables)
                        logger.info(f"æˆåŠŸä»{filename}æå–{len(tables)}ä¸ªè¡¨æ ¼")
                    else:
                        logger.warning(f"æœªä»{filename}æå–åˆ°è¡¨æ ¼")
                        failed_files.append((filename, "æœªæå–åˆ°è¡¨æ ¼"))

                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {filename}, é”™è¯¯: {str(e)}")
                    failed_files.append((filename, f"å¤„ç†å¤±è´¥: {str(e)}"))
                    continue

            if all_tables:
                output_path = self.merge_tables(all_tables)
                if output_path:
                    logger.info(f"æˆåŠŸåˆå¹¶è¡¨æ ¼åˆ°: {output_path}")
                    return output_path, failed_files
                else:
                    logger.error("åˆå¹¶è¡¨æ ¼å¤±è´¥")
                    return None, failed_files + [("merge", "åˆå¹¶è¡¨æ ¼å¤±è´¥")]
            else:
                logger.warning("æ²¡æœ‰å¯åˆå¹¶çš„è¡¨æ ¼")
                return None, failed_files

        except Exception as e:
            logger.error(f"å¤„ç†PDFæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None, [("process", f"å¤„ç†å¤±è´¥: {str(e)}")]

def get_download_link(file_path, link_text="ä¸‹è½½Excelæ–‡ä»¶"):
    """ä¸ºæ–‡ä»¶åˆ›å»ºä¸€ä¸ªä¸‹è½½é“¾æ¥"""
    try:
        with open(file_path, "rb") as f:
            data = f.read()
        b64 = base64.b64encode(data).decode()
        filename = os.path.basename(file_path)
        href = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" download="{filename}">{link_text}</a>'
        return href
    except Exception as e:
        logger.error(f"åˆ›å»ºä¸‹è½½é“¾æ¥å¤±è´¥: {e}")
        return None

def main():
    st.set_page_config(
        page_title="PDFè¡¨æ ¼æå–å·¥å…·",
        page_icon="ğŸ“Š",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("PDFè¡¨æ ¼æå–å·¥å…·")
    st.markdown("""
    ### å°†PDFæ–‡ä»¶ä¸­çš„è¡¨æ ¼æå–åˆ°Excel
    
    ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªPDFæ–‡ä»¶ï¼Œè‡ªåŠ¨æå–å…¶ä¸­çš„è¡¨æ ¼å¹¶ä¿å­˜åˆ°Excelæ–‡ä»¶ä¸­ã€‚æ‰€æœ‰è¡¨æ ¼å°†æŒ‰é¡ºåºä¿å­˜åœ¨åŒä¸€ä¸ªsheetä¸­ã€‚
    
    **æ”¯æŒåŠŸèƒ½**:
    - æ‰¹é‡å¤„ç†å¤šä¸ªPDFæ–‡ä»¶
    - è‡ªåŠ¨è¯†åˆ«å’Œæå–è¡¨æ ¼
    - ä¿ç•™å®Œæ•´çš„åˆ—åä¿¡æ¯
    - æ¥æºæ–‡ä»¶å’Œè¡¨æ ¼åºå·å•ç‹¬åˆ—ç¤º
    - å¯¼å‡ºä¸ºExcelæ ¼å¼ï¼Œä¾¿äºæŸ¥çœ‹å’Œç¼–è¾‘
    """)
    
    processor = PDFProcessor(UPLOAD_FOLDER)
    
    uploaded_files = st.file_uploader("é€‰æ‹©PDFæ–‡ä»¶", type="pdf", accept_multiple_files=True)
    
    if uploaded_files:
        st.write(f"å·²é€‰æ‹© {len(uploaded_files)} ä¸ªæ–‡ä»¶")
        
        if st.button("å¼€å§‹å¤„ç†"):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            try:
                status_text.text("æ­£åœ¨å¤„ç†æ–‡ä»¶...")
                output_path, failed_files = processor.process_pdfs(uploaded_files)
                
                progress_bar.progress(100)
                
                if output_path:
                    st.success(f"å¤„ç†å®Œæˆ! æˆåŠŸæå–è¡¨æ ¼å¹¶ä¿å­˜åˆ°Excelæ–‡ä»¶ã€‚")
                    
                    download_link = get_download_link(output_path)
                    if download_link:
                        st.markdown(download_link, unsafe_allow_html=True)
                    else:
                        st.error("åˆ›å»ºä¸‹è½½é“¾æ¥å¤±è´¥")
                    
                    try:
                        preview_df = pd.read_excel(output_path)
                        st.subheader("æ•°æ®é¢„è§ˆ")
                        st.dataframe(preview_df.head(20))
                    except Exception as e:
                        st.error(f"æ— æ³•é¢„è§ˆæ•°æ®: {e}")
                
                if failed_files:
                    st.warning(f"æœ‰ {len(failed_files)} ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥")
                    failure_df = pd.DataFrame(failed_files, columns=["æ–‡ä»¶", "åŸå› "])
                    st.dataframe(failure_df)
            
            except Exception as e:
                st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
                logger.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            
            finally:
                try:
                    processor.cleanup()
                except Exception as e:
                    logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
    
    with st.expander("ä½¿ç”¨è¯´æ˜"):
        st.markdown("""
        #### å¦‚ä½•ä½¿ç”¨
        
        1. ç‚¹å‡»"é€‰æ‹©PDFæ–‡ä»¶"ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªPDFæ–‡ä»¶
        2. ç‚¹å‡»"å¼€å§‹å¤„ç†"æŒ‰é’®
        3. ç­‰å¾…å¤„ç†å®Œæˆ
        4. ç‚¹å‡»"ä¸‹è½½Excelæ–‡ä»¶"è·å–ç»“æœ
        
        #### æ³¨æ„äº‹é¡¹
        
        - æ”¯æŒçš„æœ€å¤§æ–‡ä»¶å¤§å°ä¸º32MB
        - ä»…æ”¯æŒåŒ…å«è¡¨æ ¼çš„PDFæ–‡ä»¶
        - å¤„ç†å¤§æ–‡ä»¶å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
        - æ‰€æœ‰è¡¨æ ¼å°†æŒ‰é¡ºåºä¿å­˜åœ¨åŒä¸€ä¸ªsheetä¸­
        - æ¥æºæ–‡ä»¶å’Œè¡¨æ ¼åºå·å°†æ˜¾ç¤ºåœ¨æ¯è¡Œçš„å‰ä¸¤åˆ—
        - å¦‚æœè¡¨æ ¼æå–å¤±è´¥ï¼Œè¯·å°è¯•ä½¿ç”¨æ›´æ¸…æ™°çš„PDFæ–‡ä»¶
        """)
    
    st.markdown("---")
    st.markdown("PDFè¡¨æ ¼æå–å·¥å…· | ç‰ˆæœ¬ 1.0")

if __name__ == "__main__":
    main()
